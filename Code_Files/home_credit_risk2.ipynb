{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.functions import col, lit, max, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructField, StructType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"SparkSQL\")\\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 2000)\\\n",
    "    .config(\"spark.driver.memory\", \"30g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import keras as kr\n",
    "\n",
    "#Setting the data path\n",
    "dataPath = '../Data/parquet_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_table_dtypes(df: DataFrame) -> DataFrame:\n",
    "    for col_name in df.columns:\n",
    "        # Last letter of column name will help you determine the type\n",
    "        if col_name[-1] in (\"P\", \"A\"):\n",
    "            df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dummy_data(df: DataFrame, categorical_cols: list) -> DataFrame:\n",
    "    # Stages in our Pipeline\n",
    "    stages = []\n",
    "    \n",
    "    for categorical_col in categorical_cols:\n",
    "        # Convert string to index using StringIndexer\n",
    "        string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_Index\")\n",
    "        # Encode the index as one-hot\n",
    "        encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[categorical_col + \"_OHE\"])\n",
    "        \n",
    "        # Add stages to the Pipeline\n",
    "        stages += [string_indexer, encoder]\n",
    "    \n",
    "    # Create the Pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Fit the Pipeline to the data\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    \n",
    "    # Transform the data using the Pipeline\n",
    "    df_transformed = pipeline_model.transform(df)\n",
    "    \n",
    "    # Select the original columns and the one-hot encoded columns, dropping intermediate index columns\n",
    "    selected_cols = [col for col in df.columns] + [c + \"_OHE\" for c in categorical_cols]\n",
    "    df_final = df_transformed.select(selected_cols)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_table = spark.read.parquet(dataPath + 'train/train_base.parquet', header=True, inferSchema=True)\n",
    "\n",
    "train_1= spark.read.parquet(dataPath + 'train/train_static_0_0.parquet', header=True, inferSchema=True)\n",
    "\n",
    "train_2 = spark.read.parquet(dataPath + 'train/train_static_0_1.parquet', header=True, inferSchema=True)\n",
    "\n",
    "train_static = train_1.unionByName(train_2, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "train_static_cb = spark.read.parquet(dataPath + 'train/train_static_cb_0.parquet')\n",
    "train_person_1 = spark.read.parquet(dataPath + 'train/train_person_1.parquet')\n",
    "train_credit_bureau_b_2 = spark.read.parquet(dataPath + 'train/train_credit_bureau_b_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_basetable = spark.read.parquet(dataPath + \"test/test_base.parquet\")\n",
    "\n",
    "# test_1 = spark.read.parquet(dataPath + \"test/test_static_0_0.parquet\")\n",
    "# test_2 = spark.read.parquet(dataPath + \"test/test_static_0_1.parquet\")\n",
    "# test_3 = spark.read.parquet(dataPath + \"test/test_static_0_2.parquet\")\n",
    "\n",
    "# test_1 = set_table_dtypes(test_1)\n",
    "# test_2 = set_table_dtypes(test_2)\n",
    "# test_3 = set_table_dtypes(test_3)\n",
    "\n",
    "# test_static = test_1.unionByName(test_2, allowMissingColumns=True)\n",
    "# test_static = test_static.unionByName(test_3, allowMissingColumns=True)\n",
    "\n",
    "# test_static_cb = spark.read.parquet(dataPath + \"test/test_static_cb_0.parquet\")\n",
    "# test_person_1 = spark.read.parquet(dataPath + \"test/test_person_1.parquet\")\n",
    "# test_credit_bureau_b_2 = spark.read.parquet(dataPath + \"test/test_credit_bureau_b_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_person_1_feats_1 = train_person_1.groupBy(\"case_id\").agg(\n",
    "    F.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
    "    F.max(F.when(F.col(\"incometype_1044T\") == \"SELFEMPLOYED\", 1).otherwise(0)).alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    ")\n",
    "\n",
    "train_person_1_feats_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]) \\\n",
    "                                        .filter(col(\"num_group1\") == 0) \\\n",
    "                                        .drop(\"num_group1\") \\\n",
    "                                        .withColumnRenamed(\"housetype_905L\", \"person_housetype\")\n",
    "\n",
    "\n",
    "train_credit_bureau_b_2_feats = train_credit_bureau_b_2.groupBy(\"case_id\").agg(\n",
    "    max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "    max(when(col(\"pmts_dpdvalue_108P\") > 31, 1).otherwise(0)).alias(\"pmts_dpdvalue_108P_over31\")\n",
    ")\n",
    "\n",
    "selected_static_cols = [col for col in train_static.columns if col.endswith('A') or col.endswith('M')]\n",
    "selected_static_cb_cols = [col for col in train_static_cb.columns if col.endswith('A') or col.endswith('M')]\n",
    "\n",
    "data = train_base_table.join(train_static.select([\"case_id\"] + selected_static_cols), on=\"case_id\", how=\"left\") \\\n",
    "                      .join(train_static_cb.select([\"case_id\"] + selected_static_cb_cols), on=\"case_id\", how=\"left\") \\\n",
    "                      .join(train_person_1_feats_1, on=\"case_id\", how=\"left\") \\\n",
    "                      .join(train_person_1_feats_2, on=\"case_id\", how=\"left\") \\\n",
    "                      .join(train_credit_bureau_b_2_feats, on=\"case_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_table_dtypes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_person_1_feats_1 = test_person_1_df.groupBy(\"case_id\").agg(\n",
    "#     F.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
    "#     F.max(F.when(F.col(\"incometype_1044T\") == \"SELFEMPLOYED\", 1).otherwise(0)).alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    "# )\n",
    "\n",
    "# test_person_1_feats_2 = test_person_1_df.select([\"case_id\", \"num_group1\", \"housetype_905L\"]) \\\n",
    "#                                       .filter(F.col(\"num_group1\") == 0) \\\n",
    "#                                       .drop(\"num_group1\") \\\n",
    "#                                       .withColumnRenamed(\"housetype_905L\", \"person_housetype\")\n",
    "\n",
    "# test_credit_bureau_b_2_feats = test_credit_bureau_b_2_df.groupBy(\"case_id\").agg(\n",
    "#     F.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "#     F.max(F.when(F.col(\"pmts_dpdvalue_108P\") > 31, 1).otherwise(0)).alias(\"pmts_dpdvalue_108P_over31\")\n",
    "# )\n",
    "\n",
    "# data_submission = test_basetable_df.join(\n",
    "#     test_static_df.select([\"case_id\"] + selected_static_cols), on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_static_cb_df.select([\"case_id\"] + selected_static_cb_cols), on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_1, on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_2, on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_credit_bureau_b_2_feats, on=\"case_id\", how=\"left\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy = dummy_data(data, selected_static_cols + selected_static_cb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas_in_chunks(spark_df, chunk_size=100):\n",
    "    pandas_df_list = []\n",
    "    total_rows = spark_df.count()\n",
    "    num_chunks = total_rows // chunk_size + 1\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        pandas_df_list.append(spark_df.limit(chunk_size).toPandas())\n",
    "        spark_df = spark_df.subtract(spark_df.limit(chunk_size))\n",
    "    \n",
    "    # Concatenate all small Pandas DataFrames into one\n",
    "    return pd.concat(pandas_df_list, ignore_index=True)\n",
    "\n",
    "large_pandas_df = to_pandas_in_chunks(data_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_dummy.select(\"target\")\n",
    "X = data_dummy.drop(\"target\")\n",
    "case_ids_train = X.randomSplit()\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train, X_train, y_train = case_ids_train\n",
    "base_valid, X_valid, y_valid = case_ids_valid\n",
    "base_test, X_test, y_test = case_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    activation = hp.Choice('activation', ['tanh', 'sigmoid', 'leaky_relu', 'elu', 'selu', 'PReLU'])\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "                                        min_value=1,\n",
    "                                        max_value=30,\n",
    "                                        step=2), activation=activation, input_dim=len(X_scaled[0])))\n",
    "    for i in range(hp.Int('num_layers', 1, 10)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                        min_value=1,\n",
    "                                        max_value=30,\n",
    "                                        step=2), activation=activation))\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    loss = hp.Choice('loss', ['binary_crossentropy', 'mse'])\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam'])\n",
    "    nn_model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(create_model, objective='val_accuracy', max_epochs=20, hyperband_iterations=10, directory='my_dir', project_name='home_credit_risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model against full test data\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_scaled[0])\n",
    "hidden_nodes_layer1 =  21\n",
    "hidden_nodes_layer2 = 17\n",
    "hidden_nodes_layer3 = 23\n",
    "hidden_nodes_layer4 = 23\n",
    "hidden_nodes_layer5 = 15\n",
    "hidden_nodes_layer6 = 11\n",
    "hidden_nodes_layer7 = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"selu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer7, activation=\"selu\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(output_dim, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn_model.fit(X_scaled, y_train, epochs=100, initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc = roc(y_test, predictions)\n",
    "print(f'AUC-ROC score: {auc_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save('home_credit_risk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "    y_pred = xgb.predict(X)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc(base_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "print(f'The AUC score on the test set is: {roc(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
