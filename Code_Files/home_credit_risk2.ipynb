{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import keras as kr\n",
    "import gc\n",
    "#Setting the data path\n",
    "dataPath = '../Data/parquet_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_table_dtypes(df):\n",
    "    for col_name in df.columns:\n",
    "        if col_name[-1] in (\"P\", \"A\"):\n",
    "            df[col_name] = df[col_name].astype(float)\n",
    "    return df\n",
    "\n",
    "def dummy_data(df, categorical_cols):\n",
    "    for categorical_col in categorical_cols:\n",
    "        dummies = pd.get_dummies(df[categorical_col], prefix=categorical_col)\n",
    "        df = pd.concat([df, dummies], axis=1).drop(columns=[categorical_col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the base table\n",
    "train_base_table = pd.read_parquet(dataPath + 'train/train_base.parquet')\n",
    "\n",
    "# Reading the first part of the static table\n",
    "train_1 = pd.read_parquet(dataPath + 'train/train_static_0_0.parquet')\n",
    "\n",
    "# Reading the second part of the static table\n",
    "train_2 = pd.read_parquet(dataPath + 'train/train_static_0_1.parquet')\n",
    "\n",
    "# Combining the two parts of the static table\n",
    "# Ensure that both DataFrames have the same columns, filling missing ones with NaN\n",
    "columns_union = train_1.columns.union(train_2.columns, sort=False)\n",
    "train_1_aligned = train_1.reindex(columns=columns_union, fill_value=pd.NA)\n",
    "train_2_aligned = train_2.reindex(columns=columns_union, fill_value=pd.NA)\n",
    "\n",
    "# Concatenating aligned DataFrames\n",
    "train_static = pd.concat([train_1_aligned, train_2_aligned], ignore_index=True)\n",
    "\n",
    "# Reading additional tables\n",
    "train_static_cb = pd.read_parquet(dataPath + 'train/train_static_cb_0.parquet')\n",
    "train_person_1 = pd.read_parquet(dataPath + 'train/train_person_1.parquet')\n",
    "train_credit_bureau_b_2 = pd.read_parquet(dataPath + 'train/train_credit_bureau_b_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_basetable = spark.read.parquet(dataPath + \"test/test_base.parquet\")\n",
    "\n",
    "# test_1 = spark.read.parquet(dataPath + \"test/test_static_0_0.parquet\")\n",
    "# test_2 = spark.read.parquet(dataPath + \"test/test_static_0_1.parquet\")\n",
    "# test_3 = spark.read.parquet(dataPath + \"test/test_static_0_2.parquet\")\n",
    "\n",
    "# test_1 = set_table_dtypes(test_1)\n",
    "# test_2 = set_table_dtypes(test_2)\n",
    "# test_3 = set_table_dtypes(test_3)\n",
    "\n",
    "# test_static = test_1.unionByName(test_2, allowMissingColumns=True)\n",
    "# test_static = test_static.unionByName(test_3, allowMissingColumns=True)\n",
    "\n",
    "# test_static_cb = spark.read.parquet(dataPath + \"test/test_static_cb_0.parquet\")\n",
    "# test_person_1 = spark.read.parquet(dataPath + \"test/test_person_1.parquet\")\n",
    "# test_credit_bureau_b_2 = spark.read.parquet(dataPath + \"test/test_credit_bureau_b_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_person_1_feats_1 = train_person_1.groupby(\"case_id\").agg(\n",
    "    mainoccupationinc_384A_max=pd.NamedAgg(column=\"mainoccupationinc_384A\", aggfunc=\"max\"),\n",
    "    mainoccupationinc_384A_any_selfemployed=pd.NamedAgg(column=\"incometype_1044T\", aggfunc=lambda x: np.max(np.where(x == \"SELFEMPLOYED\", 1, 0)))\n",
    ").reset_index()\n",
    "\n",
    "train_person_1_feats_2 = train_person_1.loc[train_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]] \\\n",
    "                                        .rename(columns={\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "\n",
    "\n",
    "train_credit_bureau_b_2_feats = train_credit_bureau_b_2.groupby(\"case_id\").agg(\n",
    "    pmts_pmtsoverdue_635A_max=pd.NamedAgg(column=\"pmts_pmtsoverdue_635A\", aggfunc=\"max\"),\n",
    "    pmts_dpdvalue_108P_over31=pd.NamedAgg(column=\"pmts_dpdvalue_108P\", aggfunc=lambda x: np.max(np.where(x > 31, 1, 0)))\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Selecting columns that end with 'A' or 'M'\n",
    "selected_static_cols = [col for col in train_static.columns if col.endswith('A') or col.endswith('M')]\n",
    "selected_static_cb_cols = [col for col in train_static_cb.columns if col.endswith('A') or col.endswith('M')]\n",
    "\n",
    "# Joining DataFrames\n",
    "data = train_base_table.merge(train_static[[\"case_id\"] + selected_static_cols], on=\"case_id\", how=\"left\") \\\n",
    "                       .merge(train_static_cb[[\"case_id\"] + selected_static_cb_cols], on=\"case_id\", how=\"left\") \\\n",
    "                       .merge(train_person_1_feats_1, on=\"case_id\", how=\"left\") \\\n",
    "                       .merge(train_person_1_feats_2, on=\"case_id\", how=\"left\") \\\n",
    "                       .merge(train_credit_bureau_b_2_feats, on=\"case_id\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_table_dtypes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1526659, 58)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_person_1_feats_1 = test_person_1_df.groupBy(\"case_id\").agg(\n",
    "#     F.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
    "#     F.max(F.when(F.col(\"incometype_1044T\") == \"SELFEMPLOYED\", 1).otherwise(0)).alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    "# )\n",
    "\n",
    "# test_person_1_feats_2 = test_person_1_df.select([\"case_id\", \"num_group1\", \"housetype_905L\"]) \\\n",
    "#                                       .filter(F.col(\"num_group1\") == 0) \\\n",
    "#                                       .drop(\"num_group1\") \\\n",
    "#                                       .withColumnRenamed(\"housetype_905L\", \"person_housetype\")\n",
    "\n",
    "# test_credit_bureau_b_2_feats = test_credit_bureau_b_2_df.groupBy(\"case_id\").agg(\n",
    "#     F.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "#     F.max(F.when(F.col(\"pmts_dpdvalue_108P\") > 31, 1).otherwise(0)).alias(\"pmts_dpdvalue_108P_over31\")\n",
    "# )\n",
    "\n",
    "# data_submission = test_basetable_df.join(\n",
    "#     test_static_df.select([\"case_id\"] + selected_static_cols), on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_static_cb_df.select([\"case_id\"] + selected_static_cb_cols), on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_1, on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_2, on=\"case_id\", how=\"left\"\n",
    "# ).join(\n",
    "#     test_credit_bureau_b_2_feats, on=\"case_id\", how=\"left\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(dataPath + '/train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(dataPath + '/train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"target\"]\n",
    "X = data.drop(columns=[\"target\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.2 GiB for an array with shape (1068661, 1536) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 2\u001b[0m X_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m X_scaler\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m      4\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m X_scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:958\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\utils\\extmath.py:1069\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     new_unnormalized_variance \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1066\u001b[0m         np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1067\u001b[0m     )\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1069\u001b[0m     correction \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_accumulator_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43msum_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m     temp \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1071\u001b[0m     new_unnormalized_variance \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(sum_op, temp, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\utils\\extmath.py:967\u001b[0m, in \u001b[0;36m_safe_accumulator_op\u001b[1;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:722\u001b[0m, in \u001b[0;36mnansum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nansum_dispatcher)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnansum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m    625\u001b[0m            initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    Return the sum of array elements over a given axis treating Not a\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    Numbers (NaNs) as zero.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    720\u001b[0m \n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 722\u001b[0m     a, mask \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m    724\u001b[0m                   initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\ml_main\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:107\u001b[0m, in \u001b[0;36m_replace_nan\u001b[1;34m(a, val)\u001b[0m\n\u001b[0;32m    104\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     np\u001b[38;5;241m.\u001b[39mcopyto(a, val, where\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a, mask\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.2 GiB for an array with shape (1068661, 1536) and data type float64"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "    activation = hp.Choice('activation', ['tanh', 'sigmoid', 'leaky_relu', 'elu', 'selu', 'PReLU'])\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "                                        min_value=1,\n",
    "                                        max_value=100,\n",
    "                                        step=2), activation=activation, input_dim=len(X_scaled[0])))\n",
    "    for i in range(hp.Int('num_layers', 1, 10)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                        min_value=1,\n",
    "                                        max_value=100,\n",
    "                                        step=2), activation=activation))\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    loss = hp.Choice('loss', ['binary_crossentropy', 'mse'])\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam'])\n",
    "    nn_model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(create_model, objective='val_accuracy', max_epochs=20, hyperband_iterations=10, directory='my_dir', project_name='home_credit_risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model against full test data\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_scaled[0])\n",
    "hidden_nodes_layer1 =  21\n",
    "hidden_nodes_layer2 = 17\n",
    "hidden_nodes_layer3 = 23\n",
    "hidden_nodes_layer4 = 23\n",
    "hidden_nodes_layer5 = 15\n",
    "hidden_nodes_layer6 = 11\n",
    "hidden_nodes_layer7 = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"selu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"selu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer7, activation=\"selu\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(output_dim, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn_model.fit(X_scaled, y_train, epochs=100, initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc = roc(y_test, predictions)\n",
    "print(f'AUC-ROC score: {auc_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save('home_credit_risk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "    y_pred = xgb.predict(X)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc(base_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "print(f'The AUC score on the test set is: {roc(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
