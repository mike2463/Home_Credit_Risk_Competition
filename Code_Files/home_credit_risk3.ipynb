{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import keras as kr\n",
    "import gc\n",
    "#Setting the data path\n",
    "dataPath = '../Data/parquet_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(dataPath + '/train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_table_dtypes(df):\n",
    "    for col_name in df.columns:\n",
    "        if col_name[-1] in (\"P\", \"A\"):\n",
    "            df[col_name] = df[col_name].astype(float)\n",
    "    return df\n",
    "\n",
    "def convert_strings_pandas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Select columns that are of type 'object' or 'string'\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    \n",
    "    # Apply transformation to each selected column\n",
    "    for col in string_cols:\n",
    "        # Convert column to 'category' type\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        \n",
    "        # Get current categories and add \"Unknown\"\n",
    "        new_categories = df[col].cat.categories.tolist() + [\"Unknown\"]\n",
    "        \n",
    "        # Define new CategoricalDtype with \"Unknown\" included\n",
    "        new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "        \n",
    "        # Convert column to new dtype\n",
    "        df[col] = df[col].astype(new_dtype)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_table_dtypes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1526659, 58)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_strings_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepar_data_set(data_df):\n",
    "    category_features = data_df.select_dtypes('category').columns.tolist()\n",
    "    numeric_features = data_df.select_dtypes('number').columns.tolist()\n",
    "    \n",
    "    for col in category_features:\n",
    "        encoder = LabelEncoder()\n",
    "        # Use 'fit_transform' to transform the column\n",
    "        data_df[col] = encoder.fit_transform(data_df[col].astype(str))\n",
    "    \n",
    "    # Return the modified DataFrame and lists of features\n",
    "    return data_df, category_features, numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,cat_features,num_feature = prepar_data_set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the unique case_ids into train, test, and validation sets\n",
    "case_ids = train[\"case_id\"].unique()\n",
    "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "# Assuming 'cols_pred' should contain all numerical features except 'case_id', 'WEEK_NUM', and 'target'\n",
    "cols_pred = [col for col in num_feature if col not in ['case_id', 'WEEK_NUM', 'target']]\n",
    "\n",
    "def from_ids_to_dataframes(case_ids):\n",
    "    filtered_data = train[train[\"case_id\"].isin(case_ids)]\n",
    "    base_data = filtered_data[['case_id'] + ['WEEK_NUM'] + ['target']]\n",
    "    X_data = filtered_data[cols_pred]\n",
    "    y_data = filtered_data[\"target\"]\n",
    "    return base_data, X_data, y_data\n",
    "\n",
    "base_train, X_train, y_train = from_ids_to_dataframes(case_ids_train)\n",
    "base_valid, X_valid, y_valid = from_ids_to_dataframes(case_ids_valid)\n",
    "base_test, X_test, y_test = from_ids_to_dataframes(case_ids_test)\n",
    "\n",
    "# Ensure that num_feature only contains columns present in X_train\n",
    "num_feature = [col for col in num_feature if col in X_train.columns]\n",
    "cat_features = [col for col in cat_features if col in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[num_feature] = scaler.fit_transform(X_train[num_feature])\n",
    "X_valid[num_feature] = scaler.transform(X_valid[num_feature])\n",
    "X_test[num_feature] = scaler.transform(X_test[num_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_feature_tf(categorical_features,num_features,data):\n",
    "    models= []\n",
    "    inputs = []\n",
    "    for cat in categorical_features:\n",
    "        vocab_size = data[cat].nunique()\n",
    "        inpt = tf.keras.layers.Input(shape=(1,),name='input_'+'_'.join(cat.split(' ')))\n",
    "        inputs.append(inpt)\n",
    "        embed = tf.keras.layers.Embedding(vocab_size,200,\\\n",
    "                                          trainable=True,embeddings_initializer=tf.initializers.random_normal)(inpt)\n",
    "        embed_rehsaped =tf.keras.layers.Reshape(target_shape=(200,))(embed)\n",
    "        models.append(embed_rehsaped)\n",
    "    num_input = tf.keras.layers.Input(shape=(len(num_features)),\\\n",
    "                                      name='input_number_features')\n",
    "    inputs.append(num_input)\n",
    "    models.append(num_input)\n",
    "    merge_models= tf.keras.layers.concatenate(models)\n",
    "    pre_preds = tf.keras.layers.Dense(1000)(merge_models)\n",
    "    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n",
    "    pre_preds = tf.keras.layers.Dense(1000)(pre_preds)\n",
    "    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n",
    "    pred = tf.keras.layers.Dense(1,activation='sigmoid')(pre_preds)\n",
    "    model_full = tf.keras.models.Model(inputs= inputs,\\\n",
    "                                       outputs =pred)\n",
    "    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n",
    "                       metrics=['accuracy'],\n",
    "                       optimizer='adam')\n",
    "    return model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_categorical_feature_tf(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = model.fit(X_train, y_train, epochs=100, initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = model.evaluate(X_valid,y_valid,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc = roc(y_test, predictions)\n",
    "print(f'AUC-ROC score: {auc_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('home_credit_risk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "    y_pred = model.predict(X)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc(base_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "print(f'The AUC score on the test set is: {roc(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
