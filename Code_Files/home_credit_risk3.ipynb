{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import keras as kr\n",
    "import gc\n",
    "#Setting the data path\n",
    "dataPath = '../Data/parquet_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_table_dtypes(df):\n",
    "    for col_name in df.columns:\n",
    "        if col_name[-1] in (\"P\", \"A\"):\n",
    "            df[col_name] = df[col_name].astype(float)\n",
    "    return df\n",
    "\n",
    "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Select columns that are of type 'object' or 'string'\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    \n",
    "    # Apply transformation to each selected column\n",
    "    for col in string_cols:\n",
    "        # Convert column to 'category' type\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        \n",
    "        # Get current categories and add \"Unknown\"\n",
    "        new_categories = df[col].cat.categories.tolist() + [\"Unknown\"]\n",
    "        \n",
    "        # Define new CategoricalDtype with \"Unknown\" included\n",
    "        new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "        \n",
    "        # Convert column to new dtype\n",
    "        df[col] = df[col].astype(new_dtype)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the base table\n",
    "train_base_table = pd.read_parquet(dataPath + 'train/train_base.parquet')\n",
    "\n",
    "# Reading the first part of the static table\n",
    "train_1 = pd.read_parquet(dataPath + 'train/train_static_0_0.parquet')\n",
    "\n",
    "# Reading the second part of the static table\n",
    "train_2 = pd.read_parquet(dataPath + 'train/train_static_0_1.parquet')\n",
    "\n",
    "# Combining the two parts of the static table\n",
    "# Ensure that both DataFrames have the same columns, filling missing ones with NaN\n",
    "columns_union = train_1.columns.union(train_2.columns, sort=False)\n",
    "train_1_aligned = train_1.reindex(columns=columns_union, fill_value=pd.NA)\n",
    "train_2_aligned = train_2.reindex(columns=columns_union, fill_value=pd.NA)\n",
    "\n",
    "# Concatenating aligned DataFrames\n",
    "train_static = pd.concat([train_1_aligned, train_2_aligned], ignore_index=True)\n",
    "\n",
    "# Reading additional tables\n",
    "train_static_cb = pd.read_parquet(dataPath + 'train/train_static_cb_0.parquet')\n",
    "train_person_1 = pd.read_parquet(dataPath + 'train/train_person_1.parquet')\n",
    "train_credit_bureau_b_2 = pd.read_parquet(dataPath + 'train/train_credit_bureau_b_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations for train_person_1_feats_1\n",
    "train_person_1_feats_1 = train_person_1.groupby('case_id').agg(\n",
    "    mainoccupationinc_384A_max=('mainoccupationinc_384A', 'max'),\n",
    "    mainoccupationinc_384A_any_selfemployed=('incometype_1044T', lambda x: np.max(np.where(x == \"SELFEMPLOYED\", 1, 0)))\n",
    ").reset_index()\n",
    "\n",
    "# Filtering and renaming for train_person_1_feats_2\n",
    "train_person_1_feats_2 = train_person_1[train_person_1['num_group1'] == 0][['case_id', 'housetype_905L']] \\\n",
    "                                        .rename(columns={'housetype_905L': 'person_housetype'})\n",
    "\n",
    "# Aggregations for train_credit_bureau_b_2_feats\n",
    "train_credit_bureau_b_2_feats = train_credit_bureau_b_2.groupby('case_id').agg(\n",
    "    pmts_pmtsoverdue_635A_max=('pmts_pmtsoverdue_635A', 'max'),\n",
    "    pmts_dpdvalue_108P_over31=('pmts_dpdvalue_108P', lambda x: np.max(np.where(x > 31, 1, 0)))\n",
    ").reset_index()\n",
    "\n",
    "# Selecting columns that end with 'A' or 'M'\n",
    "selected_static_cols = [col for col in train_static.columns if col.endswith('A') or col.endswith('M')]\n",
    "selected_static_cb_cols = [col for col in train_static_cb.columns if col.endswith('A') or col.endswith('M')]\n",
    "\n",
    "# Joining DataFrames\n",
    "data = train_base_table.merge(train_static[['case_id'] + selected_static_cols], on='case_id', how='left') \\\n",
    "                       .merge(train_static_cb[['case_id'] + selected_static_cb_cols], on='case_id', how='left') \\\n",
    "                       .merge(train_person_1_feats_1, on='case_id', how='left') \\\n",
    "                       .merge(train_person_1_feats_2, on='case_id', how='left') \\\n",
    "                       .merge(train_credit_bureau_b_2_feats, on='case_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_table_dtypes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_id', 'MONTH', 'WEEK_NUM', 'target',\n",
       "       'amtinstpaidbefduel24m_4187115A', 'annuity_780A',\n",
       "       'annuitynextmonth_57A', 'avginstallast24m_3658937A',\n",
       "       'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A',\n",
       "       'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A',\n",
       "       'currdebtcredtyperange_828A', 'disbursedcredamount_1113A',\n",
       "       'downpmt_116A', 'inittransactionamount_650A', 'lastapprcredamount_781A',\n",
       "       'lastotherinc_902A', 'lastotherlnsexpense_631A',\n",
       "       'lastrejectcredamount_222A', 'maininc_215A', 'maxannuity_159A',\n",
       "       'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A',\n",
       "       'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A',\n",
       "       'maxpmtlast3m_4525190A', 'price_1097A', 'sumoutstandtotal_3546847A',\n",
       "       'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A',\n",
       "       'totinstallast1m_4525188A', 'pmtaverage_3A', 'pmtaverage_4527227A',\n",
       "       'pmtaverage_4955615A', 'pmtssum_45A', 'mainoccupationinc_384A_max',\n",
       "       'mainoccupationinc_384A_any_selfemployed', 'pmts_pmtsoverdue_635A_max',\n",
       "       'pmts_dpdvalue_108P_over31'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_strings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../Data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_79724\\488770594.py:1: DtypeWarning: Columns (55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../Data/train.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1526659, 58)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_list = data.select_dtypes('category').columns\n",
    "cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepar_data_set(data_df):\n",
    "    category_features = data_df.select_dtypes('category').columns.tolist()\n",
    "    numeric_features = data_df.select_dtypes('number').columns.tolist()\n",
    "    \n",
    "    for col in category_features:\n",
    "        encoder = LabelEncoder()\n",
    "        # Use 'fit_transform' to transform the column\n",
    "        data_df[col] = encoder.fit_transform(data_df[col].astype(str))\n",
    "    \n",
    "    # Return the modified DataFrame and lists of features\n",
    "    return data_df, category_features, numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,cat_features,num_feature = prepar_data_set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "      <th>amtinstpaidbefduel24m_4187115A</th>\n",
       "      <th>annuity_780A</th>\n",
       "      <th>annuitynextmonth_57A</th>\n",
       "      <th>avginstallast24m_3658937A</th>\n",
       "      <th>avglnamtstart24m_4525187A</th>\n",
       "      <th>...</th>\n",
       "      <th>maritalst_893M</th>\n",
       "      <th>pmtaverage_3A</th>\n",
       "      <th>pmtaverage_4527227A</th>\n",
       "      <th>pmtaverage_4955615A</th>\n",
       "      <th>pmtssum_45A</th>\n",
       "      <th>mainoccupationinc_384A_max</th>\n",
       "      <th>mainoccupationinc_384A_any_selfemployed</th>\n",
       "      <th>person_housetype</th>\n",
       "      <th>pmts_pmtsoverdue_635A_max</th>\n",
       "      <th>pmts_dpdvalue_108P_over31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1917.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4937.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4643.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3390.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  target  \\\n",
       "0        0    2019-01-03  201901         0       0   \n",
       "1        1    2019-01-03  201901         0       0   \n",
       "2        2    2019-01-04  201901         0       0   \n",
       "3        3    2019-01-03  201901         0       0   \n",
       "4        4    2019-01-04  201901         0       1   \n",
       "\n",
       "   amtinstpaidbefduel24m_4187115A  annuity_780A  annuitynextmonth_57A  \\\n",
       "0                             NaN        1917.6                   0.0   \n",
       "1                             NaN        3134.0                   0.0   \n",
       "2                             NaN        4937.0                   0.0   \n",
       "3                             NaN        4643.6                   0.0   \n",
       "4                             NaN        3390.2                   0.0   \n",
       "\n",
       "   avginstallast24m_3658937A  avglnamtstart24m_4525187A  ...  maritalst_893M  \\\n",
       "0                        NaN                        NaN  ...             NaN   \n",
       "1                        NaN                        NaN  ...             NaN   \n",
       "2                        NaN                        NaN  ...             NaN   \n",
       "3                        NaN                        NaN  ...             NaN   \n",
       "4                        NaN                        NaN  ...             NaN   \n",
       "\n",
       "   pmtaverage_3A  pmtaverage_4527227A  pmtaverage_4955615A  pmtssum_45A  \\\n",
       "0            NaN                  NaN                  NaN          NaN   \n",
       "1            NaN                  NaN                  NaN          NaN   \n",
       "2            NaN                  NaN                  NaN          NaN   \n",
       "3            NaN                  NaN                  NaN          NaN   \n",
       "4            NaN                  NaN                  NaN          NaN   \n",
       "\n",
       "   mainoccupationinc_384A_max  mainoccupationinc_384A_any_selfemployed  \\\n",
       "0                     10800.0                                        0   \n",
       "1                     10000.0                                        0   \n",
       "2                     14000.0                                        0   \n",
       "3                     10000.0                                        0   \n",
       "4                     24000.0                                        0   \n",
       "\n",
       "   person_housetype pmts_pmtsoverdue_635A_max pmts_dpdvalue_108P_over31  \n",
       "0               NaN                       NaN                       NaN  \n",
       "1               NaN                       NaN                       NaN  \n",
       "2               NaN                       NaN                       NaN  \n",
       "3               NaN                       NaN                       NaN  \n",
       "4               NaN                       NaN                       NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique case_ids\n",
    "case_ids = train[\"case_id\"].unique()\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tss = TimeSeriesSplit(n_splits=2)  # Adjust n_splits based on your requirement\n",
    "# Split the dataset into training and a combined test/validation set\n",
    "train_indices, temp_test_indices = next(iter(tss.split(case_ids)))\n",
    "case_ids_train = case_ids[train_indices]\n",
    "temp_case_ids_test = case_ids[temp_test_indices]\n",
    "\n",
    "# Split the temporary test set further into validation and test sets\n",
    "valid_indices, test_indices = next(iter(tss.split(temp_case_ids_test)))\n",
    "case_ids_valid = temp_case_ids_test[valid_indices]\n",
    "case_ids_test = temp_case_ids_test[test_indices]\n",
    "\n",
    "cols_pred = [col for col in num_feature if col not in ['case_id', 'WEEK_NUM', 'target']]\n",
    "\n",
    "def from_ids_to_dataframes(case_ids, train_df, cols_pred):\n",
    "    filtered_data = train_df[train_df[\"case_id\"].isin(case_ids)]\n",
    "    base_data = filtered_data[['case_id', 'WEEK_NUM', 'target']]\n",
    "    X_data = filtered_data[cols_pred]\n",
    "    y_data = filtered_data[\"target\"]\n",
    "    return base_data, X_data, y_data\n",
    "\n",
    "base_train, X_train, y_train = from_ids_to_dataframes(case_ids_train, train, cols_pred)\n",
    "base_valid, X_valid, y_valid = from_ids_to_dataframes(case_ids_valid, train, cols_pred)\n",
    "base_test, X_test, y_test = from_ids_to_dataframes(case_ids_test, train, cols_pred)\n",
    "\n",
    "num_feature = [col for col in num_feature if col in X_train.columns]\n",
    "cat_features = [col for col in cat_features if col in X_train.columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_79724\\926627820.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train.fillna(0, inplace=True)\n",
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_79724\\926627820.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_valid.fillna(0, inplace=True)\n",
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_79724\\926627820.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_test.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "base_test.fillna(0, inplace=True)\n",
    "base_train.fillna(0, inplace=True)\n",
    "base_valid.fillna(0, inplace=True)\n",
    "X_valid.fillna(0, inplace=True)\n",
    "X_train.fillna(0,inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "y_train.fillna(0, inplace=True)\n",
    "y_valid.fillna(0, inplace=True)\n",
    "y_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[num_feature] = scaler.fit_transform(X_train[num_feature])\n",
    "X_valid[num_feature] = scaler.transform(X_valid[num_feature])\n",
    "X_test[num_feature] = scaler.transform(X_test[num_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_feature_tf(categorical_features, num_features, data):\n",
    "    models = []\n",
    "    inputs = []\n",
    "\n",
    "    # Existing code to create embedding layers for categorical features\n",
    "    for cat in categorical_features:\n",
    "        vocab_size = data[cat].nunique()\n",
    "        inpt = tf.keras.layers.Input(shape=(1,), name='input_'+'_'.join(cat.split(' ')))\n",
    "        inputs.append(inpt)\n",
    "        embed = tf.keras.layers.Embedding(vocab_size, 500, trainable=True, embeddings_initializer=tf.initializers.random_normal())(inpt)\n",
    "        embed_reshaped = tf.keras.layers.Reshape(target_shape=(500,))(embed)\n",
    "        models.append(embed_reshaped)\n",
    "\n",
    "    # Existing code to handle numerical features\n",
    "    num_input = tf.keras.layers.Input(shape=(len(num_features)), name='input_number_features')\n",
    "    inputs.append(num_input)\n",
    "    models.append(num_input)\n",
    "\n",
    "    # Merging categorical and numerical features\n",
    "    merge_models = tf.keras.layers.concatenate(models)\n",
    "\n",
    "    # Adjusted model architecture\n",
    "    pre_preds = tf.keras.layers.Dense(512, kernel_regularizer=regularizers.l2(1e-4))(merge_models)\n",
    "    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n",
    "    pre_preds = tf.keras.layers.ReLU()(pre_preds)\n",
    "    pre_preds = tf.keras.layers.Dropout(0.25)(pre_preds)  # Adjusted dropout\n",
    "\n",
    "    pre_preds = tf.keras.layers.Dense(256, kernel_regularizer=regularizers.l2(1e-4))(pre_preds)\n",
    "    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n",
    "    pre_preds = tf.keras.layers.ReLU()(pre_preds)\n",
    "    pre_preds = tf.keras.layers.Dropout(0.25)(pre_preds)  # Adjusted dropout\n",
    "\n",
    "    # Final prediction layer remains unchanged\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(pre_preds)\n",
    "\n",
    "    # Creating the full model\n",
    "    model_full = tf.keras.models.Model(inputs=inputs, outputs=pred)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model_full.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
    "    \n",
    "    return model_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_categorical_feature_tf(cat_features,num_feature,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15903/15903 [==============================] - 165s 10ms/step - loss: 0.1512 - accuracy: 0.9688\n",
      "Epoch 2/20\n",
      "15903/15903 [==============================] - 180s 11ms/step - loss: 0.1391 - accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "15903/15903 [==============================] - 193s 12ms/step - loss: 0.1365 - accuracy: 0.9691\n",
      "Epoch 4/20\n",
      "15903/15903 [==============================] - 188s 12ms/step - loss: 0.1356 - accuracy: 0.9691\n",
      "Epoch 5/20\n",
      "15903/15903 [==============================] - 147s 9ms/step - loss: 0.1348 - accuracy: 0.9691\n",
      "Epoch 6/20\n",
      "15903/15903 [==============================] - 167s 10ms/step - loss: 0.1343 - accuracy: 0.9691\n",
      "Epoch 7/20\n",
      "15903/15903 [==============================] - 156s 10ms/step - loss: 0.1341 - accuracy: 0.9691\n",
      "Epoch 8/20\n",
      "15903/15903 [==============================] - 173s 11ms/step - loss: 0.1339 - accuracy: 0.9691\n",
      "Epoch 9/20\n",
      "15903/15903 [==============================] - 171s 11ms/step - loss: 0.1337 - accuracy: 0.9691\n",
      "Epoch 10/20\n",
      "15903/15903 [==============================] - 154s 10ms/step - loss: 0.1335 - accuracy: 0.9691\n",
      "Epoch 11/20\n",
      "15903/15903 [==============================] - 136s 9ms/step - loss: 0.1332 - accuracy: 0.9691\n",
      "Epoch 12/20\n",
      "15903/15903 [==============================] - 161s 10ms/step - loss: 0.1332 - accuracy: 0.9691\n",
      "Epoch 13/20\n",
      "15903/15903 [==============================] - 165s 10ms/step - loss: 0.1331 - accuracy: 0.9691\n",
      "Epoch 14/20\n",
      "15903/15903 [==============================] - 157s 10ms/step - loss: 0.1329 - accuracy: 0.9691\n",
      "Epoch 15/20\n",
      "15903/15903 [==============================] - 172s 11ms/step - loss: 0.1328 - accuracy: 0.9691\n",
      "Epoch 16/20\n",
      "15903/15903 [==============================] - 176s 11ms/step - loss: 0.1327 - accuracy: 0.9691\n",
      "Epoch 17/20\n",
      "15903/15903 [==============================] - 140s 9ms/step - loss: 0.1326 - accuracy: 0.9691\n",
      "Epoch 18/20\n",
      "15903/15903 [==============================] - 154s 10ms/step - loss: 0.1325 - accuracy: 0.9691\n",
      "Epoch 19/20\n",
      "15903/15903 [==============================] - 132s 8ms/step - loss: 0.1325 - accuracy: 0.9691\n",
      "Epoch 20/20\n",
      "15903/15903 [==============================] - 112s 7ms/step - loss: 0.1325 - accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = model.fit(X_train, y_train, epochs=20, initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 155649\n"
     ]
    }
   ],
   "source": [
    "total_params = model.count_params()\n",
    "print(f'Total parameters in the model: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301/5301 - 15s - loss: 0.1247 - accuracy: 0.9711 - 15s/epoch - 3ms/step\n",
      "Loss: 0.12470719963312149, Accuracy: 0.9711077213287354\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = model.evaluate(X_valid,y_valid,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301/5301 [==============================] - 10s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score: 0.7121192081317493\n"
     ]
    }
   ],
   "source": [
    "auc_roc = roc(y_test, predictions)\n",
    "print(f'AUC-ROC score: {auc_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('home_credit_risk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('home_credit_risk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15903/15903 [==============================] - 28s 2ms/step\n",
      "5301/5301 [==============================] - 9s 2ms/step\n",
      "5301/5301 [==============================] - 10s 2ms/step\n",
      "The AUC score on the train set is: 0.6871881128228371\n",
      "The AUC score on the valid set is: 0.6876219875756466\n",
      "The AUC score on the test set is: 0.7121192081317493\n"
     ]
    }
   ],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "    y_pred = model.predict(X)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc(base_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "print(f'The AUC score on the test set is: {roc(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stability score on the train set is: 0.3353169522396298\n",
      "The stability score on the valid set is: 0.34364977174952804\n",
      "The stability score on the test set is: 0.33572815572533565\n"
     ]
    }
   ],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    # Define a safe ROC calculation with a check for a single class\n",
    "    def safe_roc(target, score):\n",
    "        if len(np.unique(target)) < 2:  # Check if there's only one class\n",
    "            return 0  # Return a default value or handle as needed\n",
    "        else:\n",
    "            return 2 * roc(target, score) - 1\n",
    "\n",
    "    # Calculate Gini coefficients over time\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n",
    "        .sort_values(\"WEEK_NUM\") \\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n",
    "        .apply(lambda x: safe_roc(x[\"target\"], x[\"score\"])).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)  # Linear fit to the Gini coefficients over time\n",
    "    y_hat = a * x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    \n",
    "    # Calculate the stability score considering falling rate and residual std\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "# Assume base_train, base_valid, and base_test are previously defined DataFrame variables\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the base table\n",
    "test_basetable = pd.read_parquet(dataPath + \"test/test_base.parquet\")\n",
    "\n",
    "# Reading and concatenating the static tables\n",
    "test_1 = pd.read_parquet(dataPath + \"test/test_static_0_0.parquet\")\n",
    "test_2 = pd.read_parquet(dataPath + \"test/test_static_0_1.parquet\")\n",
    "test_3 = pd.read_parquet(dataPath + \"test/test_static_0_2.parquet\")\n",
    "test_1 = set_table_dtypes(test_1)\n",
    "test_2 = set_table_dtypes(test_2)\n",
    "test_3 = set_table_dtypes(test_3)\n",
    "# Concatenating test_1, test_2, test_3 with alignment on column names and allowing for missing columns\n",
    "test_static = pd.concat([test_1, test_2, test_3], axis=0, ignore_index=True, join='outer')\n",
    "\n",
    "# Reading additional tables\n",
    "test_static_cb = pd.read_parquet(dataPath + \"test/test_static_cb_0.parquet\")\n",
    "test_person_1 = pd.read_parquet(dataPath + \"test/test_person_1.parquet\")\n",
    "test_credit_bureau_b_2 = pd.read_parquet(dataPath + \"test/test_credit_bureau_b_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy and aggregation for test_person_1_feats_1\n",
    "test_person_1_feats_1 = test_person_1.groupby(\"case_id\").agg(\n",
    "    mainoccupationinc_384A_max=('mainoccupationinc_384A', 'max'),\n",
    "    mainoccupationinc_384A_any_selfemployed=('incometype_1044T', lambda x: (x == 'SELFEMPLOYED').max().astype(int))\n",
    ").reset_index()\n",
    "\n",
    "# Filtering, dropping, and renaming for test_person_1_feats_2\n",
    "test_person_1_feats_2 = test_person_1.loc[test_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]] \\\n",
    "                                        .rename(columns={\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "# GroupBy and aggregation for test_credit_bureau_b_2_feats\n",
    "test_credit_bureau_b_2_feats = test_credit_bureau_b_2.groupby(\"case_id\").agg(\n",
    "    pmts_pmtsoverdue_635A_max=('pmts_pmtsoverdue_635A', 'max'),\n",
    "    pmts_dpdvalue_108P_over31=('pmts_dpdvalue_108P', lambda x: (x > 31).max().astype(int))\n",
    ").reset_index()\n",
    "\n",
    "# Joining DataFrames for data_submission\n",
    "data_submission = test_basetable.merge(\n",
    "    test_static[[\"case_id\"] + selected_static_cols], on=\"case_id\", how=\"left\"\n",
    ").merge(\n",
    "    test_static_cb[[\"case_id\"] + selected_static_cb_cols], on=\"case_id\", how=\"left\"\n",
    ").merge(\n",
    "    test_person_1_feats_1, on=\"case_id\", how=\"left\"\n",
    ").merge(\n",
    "    test_person_1_feats_2, on=\"case_id\", how=\"left\"\n",
    ").merge(\n",
    "    test_credit_bureau_b_2_feats, on=\"case_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>amtinstpaidbefduel24m_4187115A</th>\n",
       "      <th>annuity_780A</th>\n",
       "      <th>annuitynextmonth_57A</th>\n",
       "      <th>avginstallast24m_3658937A</th>\n",
       "      <th>avglnamtstart24m_4525187A</th>\n",
       "      <th>avgoutstandbalancel6m_4187114A</th>\n",
       "      <th>...</th>\n",
       "      <th>maritalst_893M</th>\n",
       "      <th>pmtaverage_3A</th>\n",
       "      <th>pmtaverage_4527227A</th>\n",
       "      <th>pmtaverage_4955615A</th>\n",
       "      <th>pmtssum_45A</th>\n",
       "      <th>mainoccupationinc_384A_max</th>\n",
       "      <th>mainoccupationinc_384A_any_selfemployed</th>\n",
       "      <th>person_housetype</th>\n",
       "      <th>pmts_pmtsoverdue_635A_max</th>\n",
       "      <th>pmts_dpdvalue_108P_over31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57543</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7637.20000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57549</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>902.60004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57551</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3610.20000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57552</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6964.40000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16327.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57569</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5553.40000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16303.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57630</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7404.80030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57631</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2872.80000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16863.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57632</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6225.80030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24565.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57633</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7917.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57634</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>202010</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5894.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  amtinstpaidbefduel24m_4187115A  \\\n",
       "0    57543    2020-10-06  202010        92                             NaN   \n",
       "1    57549    2020-10-06  202010        92                             NaN   \n",
       "2    57551    2020-10-06  202010        92                             NaN   \n",
       "3    57552    2020-10-07  202010        92                             NaN   \n",
       "4    57569    2020-10-06  202010        92                             NaN   \n",
       "5    57630    2020-10-06  202010        92                             NaN   \n",
       "6    57631    2020-10-06  202010        92                             NaN   \n",
       "7    57632    2020-10-06  202010        92                             NaN   \n",
       "8    57633    2020-10-06  202010        92                             NaN   \n",
       "9    57634    2020-10-06  202010        92                             NaN   \n",
       "\n",
       "   annuity_780A  annuitynextmonth_57A  avginstallast24m_3658937A  \\\n",
       "0    7637.20000                   0.0                        NaN   \n",
       "1     902.60004                   0.0                        NaN   \n",
       "2    3610.20000                   0.0                        NaN   \n",
       "3    6964.40000                   0.0                        NaN   \n",
       "4    5553.40000                   0.0                        NaN   \n",
       "5    7404.80030                   0.0                        NaN   \n",
       "6    2872.80000                   0.0                        NaN   \n",
       "7    6225.80030                   0.0                        NaN   \n",
       "8    7917.00000                   0.0                        NaN   \n",
       "9    5894.00000                   0.0                        NaN   \n",
       "\n",
       "   avglnamtstart24m_4525187A  avgoutstandbalancel6m_4187114A  ...  \\\n",
       "0                        NaN                             NaN  ...   \n",
       "1                        NaN                             NaN  ...   \n",
       "2                        NaN                             NaN  ...   \n",
       "3                        NaN                             NaN  ...   \n",
       "4                        NaN                             NaN  ...   \n",
       "5                        NaN                             NaN  ...   \n",
       "6                        NaN                             NaN  ...   \n",
       "7                        NaN                             NaN  ...   \n",
       "8                        NaN                             NaN  ...   \n",
       "9                        NaN                             NaN  ...   \n",
       "\n",
       "   maritalst_893M  pmtaverage_3A  pmtaverage_4527227A  pmtaverage_4955615A  \\\n",
       "0        a55475b1            NaN                  NaN                  NaN   \n",
       "1        a55475b1            NaN                  NaN                  NaN   \n",
       "2        a55475b1            NaN                  NaN                  NaN   \n",
       "3        a55475b1            NaN                  NaN              16327.0   \n",
       "4        a55475b1            NaN                  NaN              16303.4   \n",
       "5        a55475b1            NaN                  NaN                  NaN   \n",
       "6        a55475b1            NaN                  NaN              16863.0   \n",
       "7        a55475b1            NaN                  NaN              24565.8   \n",
       "8        a55475b1            NaN                  NaN                  NaN   \n",
       "9        a55475b1            NaN                  NaN               6917.0   \n",
       "\n",
       "   pmtssum_45A  mainoccupationinc_384A_max  \\\n",
       "0          NaN                     36000.0   \n",
       "1          NaN                     15000.0   \n",
       "2          NaN                     24000.0   \n",
       "3          NaN                         NaN   \n",
       "4          NaN                         NaN   \n",
       "5          NaN                         NaN   \n",
       "6          NaN                         NaN   \n",
       "7          NaN                         NaN   \n",
       "8          NaN                         NaN   \n",
       "9          NaN                         NaN   \n",
       "\n",
       "   mainoccupationinc_384A_any_selfemployed person_housetype  \\\n",
       "0                                      0.0             None   \n",
       "1                                      0.0             None   \n",
       "2                                      0.0             None   \n",
       "3                                      NaN              NaN   \n",
       "4                                      NaN              NaN   \n",
       "5                                      NaN              NaN   \n",
       "6                                      NaN              NaN   \n",
       "7                                      NaN              NaN   \n",
       "8                                      NaN              NaN   \n",
       "9                                      NaN              NaN   \n",
       "\n",
       "  pmts_pmtsoverdue_635A_max  pmts_dpdvalue_108P_over31  \n",
       "0                       NaN                        NaN  \n",
       "1                       NaN                        NaN  \n",
       "2                       NaN                        NaN  \n",
       "3                       NaN                        NaN  \n",
       "4                       NaN                        NaN  \n",
       "5                       NaN                        NaN  \n",
       "6                       NaN                        NaN  \n",
       "7                       NaN                        NaN  \n",
       "8                       NaN                        NaN  \n",
       "9                       NaN                        NaN  \n",
       "\n",
       "[10 rows x 57 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_model(data_df, train_df, cat_list, cols_pred):\n",
    "    # Copy the data to avoid modifying the original dataframe\n",
    "    X_submission = data_df[cols_pred].copy()\n",
    "\n",
    "    # Initialize a LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    for col in cat_list:\n",
    "        # Fit the encoder on the training data\n",
    "        train_unique_values = train_df[col].astype(str).unique()\n",
    "        encoder.fit(train_unique_values)\n",
    "        \n",
    "        # Transform both train and submission data to ensure consistency\n",
    "        train_df[col] = encoder.transform(train_df[col].astype(str))\n",
    "        \n",
    "        # For submission data, transform known categories and set unknown to a specific code\n",
    "        # Here, we handle unknown categories by first checking if they are in the encoder classes_\n",
    "        known_classes = set(encoder.classes_)\n",
    "        X_submission[col] = X_submission[col].astype(str).apply(lambda x: x if x in known_classes else 'Unknown')\n",
    "        \n",
    "        # Update encoder with 'Unknown' to handle unseen categories\n",
    "        encoder_classes = encoder.classes_.tolist()\n",
    "        if 'Unknown' not in encoder_classes:\n",
    "            encoder_classes.append('Unknown')\n",
    "            encoder.classes_ = np.array(encoder_classes)\n",
    "        \n",
    "        # Transform submission data\n",
    "        X_submission[col] = encoder.transform(X_submission[col])\n",
    "    \n",
    "    return X_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = preprocess_data_for_model(data_submission, train, cat_features, cols_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    }
   ],
   "source": [
    "y_submission_pred = model.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission_pred = y_submission_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n",
    "    \"score\": y_submission_pred\n",
    "}).set_index('case_id')\n",
    "submission.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
